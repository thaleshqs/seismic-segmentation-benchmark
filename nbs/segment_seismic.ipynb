{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a00cf7-70d7-42d7-b3b0-b130f379bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp segment_seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd8644-9fe5-4474-a5c3-6fce4dce509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import import_ipynb\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "import libs\n",
    "from libs.loader.data_loader import *\n",
    "from libs.models.load_empty_model import load_empty_model\n",
    "from libs.metrics import RunningMetrics, EarlyStopper\n",
    "from libs.utils import store_results\n",
    "from libs.loss.cross_entropy import CrossEntropyLoss\n",
    "setattr(libs.loss, 'CrossEntropyLoss', CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243a174-fecc-4f06-8516-8ab943ac5a65",
   "metadata": {},
   "source": [
    "# Segmentation Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9be7f-9953-406e-8400-d449a8f0a114",
   "metadata": {},
   "source": [
    "## Command Line Arguments\n",
    "\n",
    "### Overview\n",
    "This script `segment_seismic.py` allows training and testing of seismic image segmentation models. It provides various command-line arguments to customize the behavior of the script, including the choice of model architecture, data paths, hyperparameters, and more.\n",
    "\n",
    "### Usage Examples\n",
    "To train a model from scratch, use the following command:\n",
    "```bash\n",
    "python3 segment_seismic.py -t -a unet -d path/to/data.npy -l path/to/labels.npy\n",
    "```\n",
    "To test a previously trained model, omit the `-t` flag and provide the path to the stored model using the `-m` option:\n",
    "```bash\n",
    "python3 segment_seismic.py -a unet -d path/to/data.npy -l path/to/labels.npy -m path/to/model.pt\n",
    "```\n",
    "\n",
    "### Command Line Arguments\n",
    "\n",
    "| Argument                | Description                                                                                                                                                                                     | Default    |\n",
    "|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|\n",
    "| `-a`, `--architecture`  | Choose a model architecture for training. Options: `segnet`, `unet`, `deconvnet`.                                                                                                              |            |\n",
    "| `-d`, `--data-path`     | Path to the seismic volume file in `.npy` or `.segy` format.                                                                                                                                    |            |\n",
    "| `-l`, `--labels-path`   | Path to the labels file in `.npy` format.                                                                                                                                                       |            |\n",
    "| `-t`, `--train`         | Train a model from scratch. If `False`, requires a path to a stored model (`-m`).                                                                                                               | `False`    |\n",
    "| `-b`, `--batch-size`    | Size of the training batch.                                                                                                                                                                     | `16`       |\n",
    "| `-D`, `--device`        | Choose GPU device for training. Defaults to CPU if GPU isn't available.                                                                                                                         | `cuda:0`   |\n",
    "| `-v`, `--cross-validation` | Enable 5-Fold Cross Validation during training.                                                                                                                                              | `False`    |\n",
    "| `-L`, `--loss-function`  | Loss function to use. Options: `cel` (Cross Entropy Loss).                                                                                                                                     | `cel`      |\n",
    "| `-o`, `--optimizer`     | Optimizer to use. Options: `adam`, `sgd` (Stochastic Gradient Descent).                                                                                                                        | `adam`     |\n",
    "| `-r`, `--learning-rate` | Learning rate for training.                                                                                                                                                                     | `1e-4`     |\n",
    "| `-w`, `--weight-decay`  | L2 regularization weight decay. `0` indicates no decay.                                                                                                                                        | `1e-5`     |\n",
    "| `-W`, `--weighted-loss` | Use class weights in the loss function to handle class imbalance.                                                                                                                               | `False`    |\n",
    "| `-e`, `--n-epochs`      | Number of training epochs.                                                                                                                                                                      | `20`       |\n",
    "| `-O`, `--orientation`    | Choose orientation for slices: `in` (inlines) or `cross` (crosslines).                                                                                                                          | `in`       |\n",
    "| `-f`, `--faulty-slices-list` | Path to a `.json` file listing indexes of faulty slices to exclude from training.                                                                                                             | `None`     |\n",
    "| `-F`, `--few-shot`      | Swap train and test sets to train on less data.                                                                                                                                                 | `False`    |\n",
    "| `-m`, `--model-path`    | Path to a stored model file (`.pt`) for testing. Ignored if training (`-t`) is enabled.                                                                                                       | `None`     |\n",
    "| `-p`, `--results-path`  | Directory for storing results. Metrics will be saved in a `.json` file, and predictions in `.png` format will be saved in a folder. If training, model weights will be saved as `.pt` files. | `results`  |\n",
    "\n",
    "### Notes\n",
    "- The `-a`, `-d`, and `-l` options are required for both training and testing.\n",
    "- When testing (`-t` is `False`), provide the path to a stored model (`-m`) to load the model weights.\n",
    "- For training, additional arguments like `-b`, `-D`, `-v`, `-L`, `-o`, `-r`, `-w`, `-W`, `-e`, `-O`, `-f`, `-F`, and `-p` can be used to configure the training process and output directory.\n",
    "- Use the `-v` option to enable 5-fold cross-validation during training.\n",
    "- If class imbalance is present, consider using `-W` to enable weighted loss.\n",
    "- `-F` can be used to train on a smaller subset of data by swapping the train and test sets.\n",
    "- The `--faulty-slices-list` option allows excluding faulty slices from the training data.\n",
    "- Results, including metrics and predictions, will be stored in the specified `results` directory.\n",
    "- In order to execute the benchmark in notebook format, you need to manually configure the arguments within the main cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3fea3-7213-4eb0-b7ae-e524e0668ba6",
   "metadata": {},
   "source": [
    "## `train_test_split`\n",
    "\n",
    "### **Description:**\n",
    "- Splits the dataset into train and test sets either using cross-validation or a simple train-test split.\n",
    "\n",
    "### **Parameters:**\n",
    "\n",
    "- `args` (object): An object containing arguments and configurations for the splitting process.\n",
    "- `dataset` (list or numpy array): The dataset to be split.\n",
    "\n",
    "### **Returns:**\n",
    "- `list`: A list of tuples where each tuple contains indices for the train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec4f10-2a4a-4706-ad2e-b64f65f92424",
   "metadata": {},
   "source": [
    "## `train`\n",
    "\n",
    "### **Description:**\n",
    "- Trains a neural network model using the provided dataset, configurations, and loss criterion.\n",
    "\n",
    "### **Parameters:**\n",
    "\n",
    "- `args` (object): An object containing training arguments and configurations.\n",
    "- `dataset` (torch Dataset): The dataset used for training.\n",
    "- `device` (torch device): The device on which to perform training (e.g., 'cpu', 'cuda').\n",
    "- `criterion` (torch.nn.Module): The loss criterion used for optimization.\n",
    "- `n_classes` (int): The number of classes in the dataset.\n",
    "- `indices` (list): List of indices indicating which examples to use for training.\n",
    "\n",
    "### **Returns:**\n",
    "- `model` (torch.nn.Module): The trained model.\n",
    "- `results` (dict): A dictionary containing training results including train scores and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff739b31-6359-421f-900d-13534cb90fb2",
   "metadata": {},
   "source": [
    "## `test`\n",
    "\n",
    "### **Description:**\n",
    "- Tests a trained neural network model using the provided dataset, configurations, and loss criterion.\n",
    "\n",
    "### **Parameters:**\n",
    "\n",
    "- `args` (object): An object containing testing arguments and configurations.\n",
    "- `dataset` (torch Dataset): The dataset used for testing.\n",
    "- `device` (torch device): The device on which to perform testing (e.g., 'cpu', 'cuda').\n",
    "- `criterion` (torch.nn.Module): The loss criterion used for evaluation.\n",
    "- `n_classes` (int): The number of classes in the dataset.\n",
    "- `indices` (list): List of indices indicating which examples to use for testing.\n",
    "- `fold` (int): The fold number used for testing in cross-validation.\n",
    "- `model` (torch.nn.Module, optional): The trained model to be tested. If None, the model is loaded from the specified model path.\n",
    "\n",
    "### **Returns:**\n",
    "- `preds` (dict): A dictionary containing the predicted outputs for each test example.\n",
    "- `results` (dict): A dictionary containing testing results including test scores and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b02dd-72f9-4383-bf72-995c9e204797",
   "metadata": {},
   "source": [
    "## `run`\n",
    "\n",
    "### **Description:**\n",
    "- Executes the training and testing pipeline based on the provided arguments and configurations.\n",
    "\n",
    "### **Parameters:**\n",
    "\n",
    "- `args` (object): An object containing all the necessary arguments and configurations for the training and testing pipeline.\n",
    "\n",
    "### **Returns:**\n",
    "- None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f46fea-89af-46e6-adbe-004387561d9c",
   "metadata": {},
   "source": [
    "## `if __name__ == '__main__':`\n",
    "\n",
    "### **Description:**\n",
    "- This block of code serves as the entry point for the script when executed directly. It defines and parses command-line arguments using the `ArgumentParser` from the `argparse` module, then calls the `run` function with the parsed arguments.\n",
    "\n",
    "### **Parameters:**\n",
    "- None\n",
    "\n",
    "### **Usage:**\n",
    "- This block of code should be placed at the end of a script file to allow for direct execution of the script with command-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f40f03-f813-4958-a601-e57a9b82b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_test_split(args, dataset):\n",
    "    if args.cross_validation:\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "        if args.few_shot:\n",
    "            # 20% train / 80% test\n",
    "            splits = [\n",
    "                (test_idx.tolist(), train_idx.tolist()) for train_idx, test_idx in kf.split(dataset)\n",
    "            ]\n",
    "        else:\n",
    "            # 80% train / 20% test\n",
    "            splits = [\n",
    "                (train_idx.tolist(), test_idx.tolist()) for train_idx, test_idx in kf.split(dataset)\n",
    "            ]\n",
    "    else:\n",
    "        if args.few_shot:\n",
    "            # 20% train / 80% test\n",
    "            test_ratio = 0.8\n",
    "        else:\n",
    "            # 80% train / 20% test\n",
    "            test_ratio = 0.2\n",
    "\n",
    "        test_size  = int(len(dataset) * test_ratio)\n",
    "        splits = [(list(range(test_size, len(dataset))), list(range(0, test_size)))]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3ba5e-d833-45eb-842b-c1fb0d06d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train(args, dataset, device, criterion, n_classes, indices):\n",
    "    print('\\nCreating model...')\n",
    "    print('Architecture:   ', args.architecture.upper())\n",
    "    print('Optimizer:      ', args.optimizer)\n",
    "    print('Device:         ', device)\n",
    "    print('Loss function:  ', args.loss_function)\n",
    "    print('Learning rate:  ', args.learning_rate)\n",
    "    print('Batch size:     ', args.batch_size)\n",
    "    print('N. of epochs:   ', args.n_epochs)\n",
    "\n",
    "    print('\\nNumber of train examples:', len(indices))\n",
    "\n",
    "    print('\\nWeighted loss ENABLED' if args.weighted_loss else 'Weighted loss DISABLED')\n",
    "    print(f'Training with {\"INLINES\" if args.orientation == \"in\" else \"CROSSLINES\"}')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model = load_empty_model(args.architecture, n_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer\n",
    "    optimizer_map = {\n",
    "        'adam': torch.optim.Adam,\n",
    "        'sgd' : torch.optim.SGD\n",
    "    }\n",
    "\n",
    "    OptimizerClass = optimizer_map[args.optimizer]\n",
    "    optimizer = OptimizerClass(\n",
    "        params=model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    # Initializing metrics\n",
    "    train_metrics = RunningMetrics(n_classes=n_classes, bf1_threshold=2)\n",
    "\n",
    "    # Storing the loss for each epoch\n",
    "    train_loss_list = []\n",
    "\n",
    "    for epoch in range(args.n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        print(datetime.now().strftime('\\n%Y/%m/%d %H:%M:%S'))\n",
    "        print(f'Training on epoch {epoch + 1}/{args.n_epochs}\\n')\n",
    "\n",
    "        for images, labels in tqdm(train_loader, ascii=' >='):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = images.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Updating the running metrics\n",
    "            train_metrics.update(images=outputs, targets=labels)\n",
    "\n",
    "            # Computing the loss and updating weights\n",
    "            loss = criterion(images=outputs, targets=labels.long())\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss = train_loss / ceil((len(train_loader) / args.batch_size))\n",
    "        train_scores = train_metrics.get_scores()\n",
    "\n",
    "        print(f'Train loss: {train_loss}')\n",
    "        print(f'Train mIoU: {train_scores[\"mean_iou\"]}')\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_metrics.reset()\n",
    "\n",
    "    results = {\n",
    "        'train_scores' : train_scores,\n",
    "        'train_losses' : train_loss_list,\n",
    "    }\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1071ee6-5da4-4ba2-9d25-d7a911f1a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test(args, dataset, device, criterion, n_classes, indices, fold, model=None):\n",
    "    print('\\nTesting model...')\n",
    "    print('Architecture:   ', args.architecture.upper())\n",
    "    print('Device:         ', device)\n",
    "    print('Loss function:  ', args.loss_function)\n",
    "    print('Batch size:     ', args.batch_size)\n",
    "\n",
    "    print(f'\\nNumber of test examples: {len(indices)} (slices {indices[0]} to {indices[-1]})')\n",
    "\n",
    "    print('\\nWeighted loss ENABLED' if args.weighted_loss else 'Weighted loss DISABLED')\n",
    "    print(f'Testing with {\"INLINES\" if args.orientation == \"in\" else \"CROSSLINES\"}')\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Initializing metrics\n",
    "    test_metrics  = RunningMetrics(n_classes=n_classes, bf1_threshold=2)\n",
    "\n",
    "    # Storing the loss for each epoch\n",
    "    test_loss_list  = []\n",
    "\n",
    "    if model is None:\n",
    "        print(f'\\nTraining is OFF. Loading stored model from {args.model_path}')\n",
    "\n",
    "        model = load_empty_model(args.architecture, n_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # model_path = os.path.join(args.model_path, f'model_fold_{fold}.pt')\n",
    "\n",
    "        if not os.path.isfile(args.model_path):\n",
    "            raise FileNotFoundError(f'No such file or directory for stored model: {args.model_path}')\n",
    "\n",
    "        # Loading previously stored model\n",
    "        model.load_state_dict(torch.load(args.model_path))\n",
    "    \n",
    "    preds = {}\n",
    "    slice_idx = indices[0]\n",
    "\n",
    "    # Testing phase\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "\n",
    "        print(datetime.now().strftime('\\n%Y/%m/%d %H:%M:%S'))\n",
    "        print('Testing the model...\\n')\n",
    "\n",
    "        for images, labels in tqdm(test_loader, ascii=' >='):\n",
    "            images = images.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Iterating over the batch\n",
    "            for pred in outputs:\n",
    "                preds[slice_idx] = pred\n",
    "                slice_idx += 1\n",
    "\n",
    "            # Updating the running metrics\n",
    "            test_metrics.update(images=outputs, targets=labels)\n",
    "\n",
    "            # Computing the loss\n",
    "            loss = criterion(images=outputs, targets=labels.long())\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / ceil((len(test_loader) / args.batch_size))\n",
    "        test_scores = test_metrics.get_scores()\n",
    "\n",
    "        print(f'Test loss: {test_loss}')\n",
    "        print(f'Test mIoU: {test_scores[\"mean_iou\"]}')\n",
    "\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_metrics.reset()\n",
    "    \n",
    "    results = {\n",
    "        'test_scores'  : test_scores,\n",
    "        'test_losses'  : test_loss_list,\n",
    "    }\n",
    "\n",
    "    return preds, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05157fc0-5f9f-4564-b5a1-5ca5f8ae4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run(args):\n",
    "    print('')\n",
    "    print('Data path:    ', args.data_path)\n",
    "    print('Labels path:  ', args.labels_path)\n",
    "    print('Results path: ', args.results_path)\n",
    "    print('')\n",
    "\n",
    "    print('Loading dataset...')\n",
    "\n",
    "    dataset = SeismicDataset(\n",
    "        data_path=args.data_path,\n",
    "        labels_path=args.labels_path,\n",
    "        orientation=args.orientation,\n",
    "        compute_weights=args.weighted_loss,\n",
    "        faulty_slices_list=args.faulty_slices_list\n",
    "    )\n",
    "\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if args.weighted_loss:\n",
    "        class_weights = torch.tensor(dataset.get_class_weights(), device=device, requires_grad=False)\n",
    "        class_weights = class_weights.float()\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    loss_map = {\n",
    "        'cel': ('CrossEntropyLoss', {'reduction': 'sum', 'weight': class_weights})\n",
    "    }\n",
    "\n",
    "    # Defining the loss function\n",
    "    loss_name, loss_args = loss_map[args.loss_function]\n",
    "    criterion = getattr(libs.loss, loss_name)(**loss_args)\n",
    "\n",
    "    # Splitting the data into train and test\n",
    "    splits = train_test_split(args, dataset)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for fold_number, (train_indices, test_indices) in enumerate(splits):\n",
    "        if args.cross_validation:\n",
    "            print(f'\\n======== FOLD {fold_number + 1}/5 ========')\n",
    "        \n",
    "        train_indices, test_indices = splits[fold_number]\n",
    "        \n",
    "        train_set = torch.utils.data.Subset(dataset, train_indices)\n",
    "        test_set = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "        if args.train:\n",
    "            model, train_results = train(\n",
    "                args,\n",
    "                train_set,\n",
    "                device=device,\n",
    "                criterion=criterion,\n",
    "                n_classes=dataset.get_n_classes(),\n",
    "                indices=train_indices\n",
    "            )\n",
    "        else:\n",
    "            model = None\n",
    "            train_results = {}\n",
    "            # train_results = load_train_results()\n",
    "\n",
    "        preds, test_results = test(\n",
    "            args,\n",
    "            test_set,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            n_classes=dataset.get_n_classes(),\n",
    "            indices=test_indices,\n",
    "            fold=fold_number,\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'preds': preds,\n",
    "            'train_indices': train_indices,\n",
    "            'test_indices': test_indices,\n",
    "            **train_results,\n",
    "            **test_results\n",
    "        })\n",
    "\n",
    "    store_results(args, results, n_classes=dataset.get_n_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f03787-7e9c-45e5-bd25-dc2fd58f3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description='Hyperparameters')\n",
    "    print(parser)\n",
    "    parser.add_argument('-a', '--architecture',\n",
    "        dest='architecture',\n",
    "        type=str,\n",
    "        help='Architecture to use [segnet, unet, deconvnet]',\n",
    "        choices=['segnet', 'unet', 'deconvnet']\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-d', '--data-path',\n",
    "        dest='data_path',\n",
    "        type=str,\n",
    "        help='Path to the data file in numpy or segy format'\n",
    "    )\n",
    "    parser.add_argument('-l', '--labels-path',\n",
    "        dest='labels_path',\n",
    "        type=str,\n",
    "        help='Path to the labels file in numpy format'\n",
    "    )\n",
    "    parser.add_argument('-t', '--train',\n",
    "        dest='train',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to train a model or to simply test from a stored model'\n",
    "    )\n",
    "    parser.add_argument('-b', '--batch-size',\n",
    "        dest='batch_size',\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help='Batch Size'\n",
    "    )\n",
    "    parser.add_argument('-D', '--device',\n",
    "        dest='device',\n",
    "        type=str,\n",
    "        default='cuda:0',\n",
    "        help='Device to train on [cuda:n]'\n",
    "    )\n",
    "    parser.add_argument('-v', '--cross-validation',\n",
    "        dest='cross_validation',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to use 5-fold cross validation'\n",
    "    )\n",
    "    parser.add_argument('-L', '--loss-function',\n",
    "        dest='loss_function',\n",
    "        type=str,\n",
    "        default='cel',\n",
    "        help='Loss function to use [cel (Cross_Entropy Loss)]',\n",
    "        choices=['cel']\n",
    "    )\n",
    "    parser.add_argument('-o', '--optimizer',\n",
    "        dest='optimizer',\n",
    "        type=str,\n",
    "        default='adam',\n",
    "        help='Optimizer to use [adam, sgd (Stochastic Gradient Descent)]',\n",
    "        choices=['adam', 'sgd']\n",
    "    )\n",
    "    parser.add_argument('-r', '--learning-rate',\n",
    "        dest='learning_rate',\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help='Learning rate'\n",
    "    )\n",
    "    parser.add_argument('-w', '--weight-decay',\n",
    "        dest='weight_decay',\n",
    "        type=float,\n",
    "        default=1e-5,\n",
    "        help='L2 regularization. Value 0 indicates no weight decay'\n",
    "    )\n",
    "    parser.add_argument('-W', '--weighted-loss',\n",
    "        dest='weighted_loss',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to use class weights in the loss function'\n",
    "    )\n",
    "    parser.add_argument('-e', '--n-epochs',\n",
    "        dest='n_epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of epochs'\n",
    "    )\n",
    "    parser.add_argument('-O', '--orientation',\n",
    "        dest='orientation',\n",
    "        type=str,\n",
    "        default='in',\n",
    "        help='Whether the model should be trained using inlines or crosslines',\n",
    "        choices=['in', 'cross']\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-f', '--faulty-slices-list',\n",
    "        dest='faulty_slices_list',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Path to a json file containing a list of faulty slices to remove'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-F', '--few-shot',\n",
    "        dest='few_shot',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to swap the train and test sets to train on less data'\n",
    "    )\n",
    "    parser.add_argument('-m', '--model-path',\n",
    "        dest='model_path',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Directory for loading saved model'\n",
    "    )\n",
    "    parser.add_argument('-p', '--results-path',\n",
    "        dest='results_path',\n",
    "        type=str,\n",
    "        default=os.path.join(os.getcwd(), 'results'),\n",
    "        help='Directory for storing execution results'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args(args=None)\n",
    "    run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
