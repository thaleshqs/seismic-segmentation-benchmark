{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e243a174-fecc-4f06-8516-8ab943ac5a65",
   "metadata": {},
   "source": [
    "# Segmentation Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a00cf7-70d7-42d7-b3b0-b130f379bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp segment_seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd8644-9fe5-4474-a5c3-6fce4dce509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import import_ipynb\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "import libs\n",
    "from libs.loader.data_loader import *\n",
    "from libs.models.load_empty_model import load_empty_model\n",
    "from libs.metrics import RunningMetrics, EarlyStopper\n",
    "from libs.utils import store_results\n",
    "from libs.loss.cross_entropy import CrossEntropyLoss\n",
    "setattr(libs.loss, 'CrossEntropyLoss', CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f40f03-f813-4958-a601-e57a9b82b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_test_split(args, dataset):\n",
    "    if args.cross_validation:\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "        if args.few_shot:\n",
    "            # 20% train / 80% test\n",
    "            splits = [\n",
    "                (test_idx.tolist(), train_idx.tolist()) for train_idx, test_idx in kf.split(dataset)\n",
    "            ]\n",
    "        else:\n",
    "            # 80% train / 20% test\n",
    "            splits = [\n",
    "                (train_idx.tolist(), test_idx.tolist()) for train_idx, test_idx in kf.split(dataset)\n",
    "            ]\n",
    "    else:\n",
    "        if args.few_shot:\n",
    "            # 20% train / 80% test\n",
    "            test_ratio = 0.8\n",
    "        else:\n",
    "            # 80% train / 20% test\n",
    "            test_ratio = 0.2\n",
    "\n",
    "        test_size  = int(len(dataset) * test_ratio)\n",
    "        splits = [(list(range(test_size, len(dataset))), list(range(0, test_size)))]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3ba5e-d833-45eb-842b-c1fb0d06d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train(args, dataset, device, criterion, n_classes, indices):\n",
    "    print('\\nCreating model...')\n",
    "    print('Architecture:   ', args.architecture.upper())\n",
    "    print('Optimizer:      ', args.optimizer)\n",
    "    print('Device:         ', device)\n",
    "    print('Loss function:  ', args.loss_function)\n",
    "    print('Learning rate:  ', args.learning_rate)\n",
    "    print('Batch size:     ', args.batch_size)\n",
    "    print('N. of epochs:   ', args.n_epochs)\n",
    "\n",
    "    print('\\nNumber of train examples:', len(indices))\n",
    "\n",
    "    print('\\nWeighted loss ENABLED' if args.weighted_loss else 'Weighted loss DISABLED')\n",
    "    print(f'Training with {\"INLINES\" if args.orientation == \"in\" else \"CROSSLINES\"}')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model = load_empty_model(args.architecture, n_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer\n",
    "    optimizer_map = {\n",
    "        'adam': torch.optim.Adam,\n",
    "        'sgd' : torch.optim.SGD\n",
    "    }\n",
    "\n",
    "    OptimizerClass = optimizer_map[args.optimizer]\n",
    "    optimizer = OptimizerClass(\n",
    "        params=model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    # Initializing metrics\n",
    "    train_metrics = RunningMetrics(n_classes=n_classes, bf1_threshold=2)\n",
    "\n",
    "    # Storing the loss for each epoch\n",
    "    train_loss_list = []\n",
    "\n",
    "    for epoch in range(args.n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        print(datetime.now().strftime('\\n%Y/%m/%d %H:%M:%S'))\n",
    "        print(f'Training on epoch {epoch + 1}/{args.n_epochs}\\n')\n",
    "\n",
    "        for images, labels in tqdm(train_loader, ascii=' >='):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = images.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Updating the running metrics\n",
    "            train_metrics.update(images=outputs, targets=labels)\n",
    "\n",
    "            # Computing the loss and updating weights\n",
    "            loss = criterion(images=outputs, targets=labels.long())\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss = train_loss / ceil((len(train_loader) / args.batch_size))\n",
    "        train_scores = train_metrics.get_scores()\n",
    "\n",
    "        print(f'Train loss: {train_loss}')\n",
    "        print(f'Train mIoU: {train_scores[\"mean_iou\"]}')\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_metrics.reset()\n",
    "\n",
    "    results = {\n",
    "        'train_scores' : train_scores,\n",
    "        'train_losses' : train_loss_list,\n",
    "    }\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1071ee6-5da4-4ba2-9d25-d7a911f1a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test(args, dataset, device, criterion, n_classes, indices, fold, model=None):\n",
    "    print('\\nTesting model...')\n",
    "    print('Architecture:   ', args.architecture.upper())\n",
    "    print('Device:         ', device)\n",
    "    print('Loss function:  ', args.loss_function)\n",
    "    print('Batch size:     ', args.batch_size)\n",
    "\n",
    "    print(f'\\nNumber of test examples: {len(indices)} (slices {indices[0]} to {indices[-1]})')\n",
    "\n",
    "    print('\\nWeighted loss ENABLED' if args.weighted_loss else 'Weighted loss DISABLED')\n",
    "    print(f'Testing with {\"INLINES\" if args.orientation == \"in\" else \"CROSSLINES\"}')\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Initializing metrics\n",
    "    test_metrics  = RunningMetrics(n_classes=n_classes, bf1_threshold=2)\n",
    "\n",
    "    # Storing the loss for each epoch\n",
    "    test_loss_list  = []\n",
    "\n",
    "    if model is None:\n",
    "        print(f'\\nTraining is OFF. Loading stored model from {args.model_path}')\n",
    "\n",
    "        model = load_empty_model(args.architecture, n_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # model_path = os.path.join(args.model_path, f'model_fold_{fold}.pt')\n",
    "\n",
    "        if not os.path.isfile(args.model_path):\n",
    "            raise FileNotFoundError(f'No such file or directory for stored model: {args.model_path}')\n",
    "\n",
    "        # Loading previously stored model\n",
    "        model.load_state_dict(torch.load(args.model_path))\n",
    "    \n",
    "    preds = {}\n",
    "    slice_idx = indices[0]\n",
    "\n",
    "    # Testing phase\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "\n",
    "        print(datetime.now().strftime('\\n%Y/%m/%d %H:%M:%S'))\n",
    "        print('Testing the model...\\n')\n",
    "\n",
    "        for images, labels in tqdm(test_loader, ascii=' >='):\n",
    "            images = images.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Iterating over the batch\n",
    "            for pred in outputs:\n",
    "                preds[slice_idx] = pred\n",
    "                slice_idx += 1\n",
    "\n",
    "            # Updating the running metrics\n",
    "            test_metrics.update(images=outputs, targets=labels)\n",
    "\n",
    "            # Computing the loss\n",
    "            loss = criterion(images=outputs, targets=labels.long())\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / ceil((len(test_loader) / args.batch_size))\n",
    "        test_scores = test_metrics.get_scores()\n",
    "\n",
    "        print(f'Test loss: {test_loss}')\n",
    "        print(f'Test mIoU: {test_scores[\"mean_iou\"]}')\n",
    "\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_metrics.reset()\n",
    "    \n",
    "    results = {\n",
    "        'test_scores'  : test_scores,\n",
    "        'test_losses'  : test_loss_list,\n",
    "    }\n",
    "\n",
    "    return preds, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05157fc0-5f9f-4564-b5a1-5ca5f8ae4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run(args):\n",
    "    print('')\n",
    "    print('Data path:    ', args.data_path)\n",
    "    print('Labels path:  ', args.labels_path)\n",
    "    print('Results path: ', args.results_path)\n",
    "    print('')\n",
    "\n",
    "    print('Loading dataset...')\n",
    "\n",
    "    dataset = SeismicDataset(\n",
    "        data_path=args.data_path,\n",
    "        labels_path=args.labels_path,\n",
    "        orientation=args.orientation,\n",
    "        compute_weights=args.weighted_loss,\n",
    "        faulty_slices_list=args.faulty_slices_list\n",
    "    )\n",
    "\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if args.weighted_loss:\n",
    "        class_weights = torch.tensor(dataset.get_class_weights(), device=device, requires_grad=False)\n",
    "        class_weights = class_weights.float()\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    loss_map = {\n",
    "        'cel': ('CrossEntropyLoss', {'reduction': 'sum', 'weight': class_weights})\n",
    "    }\n",
    "\n",
    "    # Defining the loss function\n",
    "    loss_name, loss_args = loss_map[args.loss_function]\n",
    "    criterion = getattr(libs.loss, loss_name)(**loss_args)\n",
    "\n",
    "    # Splitting the data into train and test\n",
    "    splits = train_test_split(args, dataset)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for fold_number, (train_indices, test_indices) in enumerate(splits):\n",
    "        if args.cross_validation:\n",
    "            print(f'\\n======== FOLD {fold_number + 1}/5 ========')\n",
    "        \n",
    "        train_indices, test_indices = splits[fold_number]\n",
    "        \n",
    "        train_set = torch.utils.data.Subset(dataset, train_indices)\n",
    "        test_set = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "        if args.train:\n",
    "            model, train_results = train(\n",
    "                args,\n",
    "                train_set,\n",
    "                device=device,\n",
    "                criterion=criterion,\n",
    "                n_classes=dataset.get_n_classes(),\n",
    "                indices=train_indices\n",
    "            )\n",
    "        else:\n",
    "            model = None\n",
    "            train_results = {}\n",
    "            # train_results = load_train_results()\n",
    "\n",
    "        preds, test_results = test(\n",
    "            args,\n",
    "            test_set,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            n_classes=dataset.get_n_classes(),\n",
    "            indices=test_indices,\n",
    "            fold=fold_number,\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'preds': preds,\n",
    "            'train_indices': train_indices,\n",
    "            'test_indices': test_indices,\n",
    "            **train_results,\n",
    "            **test_results\n",
    "        })\n",
    "\n",
    "    store_results(args, results, n_classes=dataset.get_n_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f03787-7e9c-45e5-bd25-dc2fd58f3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description='Hyperparameters')\n",
    "    print(parser)\n",
    "    parser.add_argument('-a', '--architecture',\n",
    "        dest='architecture',\n",
    "        type=str,\n",
    "        help='Architecture to use [segnet, unet, deconvnet]',\n",
    "        choices=['segnet', 'unet', 'deconvnet']\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-d', '--data-path',\n",
    "        dest='data_path',\n",
    "        type=str,\n",
    "        help='Path to the data file in numpy or segy format'\n",
    "    )\n",
    "    parser.add_argument('-l', '--labels-path',\n",
    "        dest='labels_path',\n",
    "        type=str,\n",
    "        help='Path to the labels file in numpy format'\n",
    "    )\n",
    "    parser.add_argument('-t', '--train',\n",
    "        dest='train',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to train a model or to simply test from a stored model'\n",
    "    )\n",
    "    parser.add_argument('-b', '--batch-size',\n",
    "        dest='batch_size',\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help='Batch Size'\n",
    "    )\n",
    "    parser.add_argument('-D', '--device',\n",
    "        dest='device',\n",
    "        type=str,\n",
    "        default='cuda:0',\n",
    "        help='Device to train on [cuda:n]'\n",
    "    )\n",
    "    parser.add_argument('-v', '--cross-validation',\n",
    "        dest='cross_validation',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to use 5-fold cross validation'\n",
    "    )\n",
    "    parser.add_argument('-L', '--loss-function',\n",
    "        dest='loss_function',\n",
    "        type=str,\n",
    "        default='cel',\n",
    "        help='Loss function to use [cel (Cross_Entropy Loss)]',\n",
    "        choices=['cel']\n",
    "    )\n",
    "    parser.add_argument('-o', '--optimizer',\n",
    "        dest='optimizer',\n",
    "        type=str,\n",
    "        default='adam',\n",
    "        help='Optimizer to use [adam, sgd (Stochastic Gradient Descent)]',\n",
    "        choices=['adam', 'sgd']\n",
    "    )\n",
    "    parser.add_argument('-r', '--learning-rate',\n",
    "        dest='learning_rate',\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help='Learning rate'\n",
    "    )\n",
    "    parser.add_argument('-w', '--weight-decay',\n",
    "        dest='weight_decay',\n",
    "        type=float,\n",
    "        default=1e-5,\n",
    "        help='L2 regularization. Value 0 indicates no weight decay'\n",
    "    )\n",
    "    parser.add_argument('-W', '--weighted-loss',\n",
    "        dest='weighted_loss',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to use class weights in the loss function'\n",
    "    )\n",
    "    parser.add_argument('-e', '--n-epochs',\n",
    "        dest='n_epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of epochs'\n",
    "    )\n",
    "    parser.add_argument('-O', '--orientation',\n",
    "        dest='orientation',\n",
    "        type=str,\n",
    "        default='in',\n",
    "        help='Whether the model should be trained using inlines or crosslines',\n",
    "        choices=['in', 'cross']\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-f', '--faulty-slices-list',\n",
    "        dest='faulty_slices_list',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Path to a json file containing a list of faulty slices to remove'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('-F', '--few-shot',\n",
    "        dest='few_shot',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to swap the train and test sets to train on less data'\n",
    "    )\n",
    "    parser.add_argument('-m', '--model-path',\n",
    "        dest='model_path',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Directory for loading saved model'\n",
    "    )\n",
    "    parser.add_argument('-p', '--results-path',\n",
    "        dest='results_path',\n",
    "        type=str,\n",
    "        default=os.path.join(os.getcwd(), 'results'),\n",
    "        help='Directory for storing execution results'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args(args=None)\n",
    "    run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
