{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa4bed-4da8-4b97-8db3-62394ac3c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp libs.loader.data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461ddb5-588f-4fc7-94d0-0e80bb78f808",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e65a89-a424-406b-9d30-51ce596a234f",
   "metadata": {},
   "source": [
    "## `SeismicDataset(Dataset)`\n",
    "\n",
    "### **Description:**\n",
    "- Represents a PyTorch Dataset for seismic data.\n",
    "\n",
    "### **Methods:**\n",
    "\n",
    " #### `__init__(self, data_path, labels_path, orientation, compute_weights=False, faulty_slices_list=None)`\n",
    "   - **Description:**\n",
    "     - Initializes the SeismicDataset.\n",
    "   - **Parameters:**\n",
    "     - `data_path` (str): Path to the seismic data file.\n",
    "     - `labels_path` (str): Path to the corresponding labels file.\n",
    "     - `orientation` (str): Orientation of the seismic data ('in' for inlines, 'crossline' for crosslines).\n",
    "     - `compute_weights` (bool): Whether to compute class weights based on frequency. Default is False.\n",
    "     - `faulty_slices_list` (str): Path to a JSON file containing a list of faulty slices to remove from the data. Default is None.\n",
    "   - **Returns:**\n",
    "     - None\n",
    "---\n",
    "\n",
    " #### `__getitem__(self, index)`\n",
    "   - **Description:**\n",
    "     - Gets an item (data sample and its label) from the dataset.\n",
    "   - **Parameters:**\n",
    "     - `index` (int): Index of the item to retrieve.\n",
    "   - **Returns:**\n",
    "     - `tuple`: A tuple containing the data sample and its label.\n",
    "\n",
    "---\n",
    "\n",
    " #### `__len__(self)`\n",
    "   - **Description:**\n",
    "     - Gets the length of the dataset.\n",
    "   - **Returns:**\n",
    "     - `int`: Length of the dataset.\n",
    "     \n",
    "---\n",
    "\n",
    " #### `get_class_weights(self)`\n",
    "   - **Description:**\n",
    "     - Gets the computed class weights.\n",
    "   - **Returns:**\n",
    "     - `numpy.ndarray`: Computed class weights.\n",
    "\n",
    "---\n",
    "\n",
    " #### `get_n_classes(self)`\n",
    "   - **Description:**\n",
    "     - Gets the number of unique classes in the dataset.\n",
    "   - **Returns:**\n",
    "     - `int`: Number of unique classes.\n",
    "     \n",
    "---\n",
    "\n",
    " #### `__load_data(self, data_path, labels_path)`\n",
    "   - **Description:**\n",
    "     - Loads seismic data and labels from files.\n",
    "   - **Parameters:**\n",
    "     - `data_path` (str): Path to the seismic data file.\n",
    "     - `labels_path` (str): Path to the corresponding labels file.\n",
    "   - **Returns:**\n",
    "     - `tuple`: A tuple containing the loaded data and labels.\n",
    "\n",
    "---\n",
    "\n",
    " #### `__compute_class_weights(self)`\n",
    "   - **Description:**\n",
    "     - Computes class weights based on frequency.\n",
    "   - **Returns:**\n",
    "     - `numpy.ndarray`: Computed class weights.\n",
    "\n",
    "---\n",
    "\n",
    " #### `__remove_faulty_slices(self, faulty_slices_list)`\n",
    "   - **Description:**\n",
    "     - Removes faulty slices from the data.\n",
    "   - **Parameters:**\n",
    "     - `faulty_slices_list` (str): Path to a JSON file containing a list of faulty slices to remove.\n",
    "   - **Returns:**\n",
    "     - None\n",
    "---\n",
    "\n",
    " #### `__process_class_labels(self)`\n",
    "   - **Description:**\n",
    "     - Processes class labels to ensure they are in the correct range.\n",
    "   - **Returns:**\n",
    "     - `int`: Number of unique classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043aa3a-6684-485a-944d-cb66b6e54f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import segyio\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SeismicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, labels_path, orientation, compute_weights=False, faulty_slices_list=None):\n",
    "        self.data, self.labels = self.__load_data(data_path, labels_path)\n",
    "        self.orientation = orientation\n",
    "\n",
    "        # Removing faulty slices from the data if specified\n",
    "        if faulty_slices_list is not None:\n",
    "            self.__remove_faulty_slices(faulty_slices_list)\n",
    "        \n",
    "        self.n_inlines, self.n_crosslines, self.n_time_slices = self.data.shape\n",
    "        \n",
    "        self.n_classes = self.__process_class_labels()\n",
    "        self.weights = self.__compute_class_weights() if compute_weights else None\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.orientation == 'in':\n",
    "            image = self.data[index, :, :]\n",
    "            label = self.labels[index, :, :]\n",
    "        else:\n",
    "            image = self.data[:, index, :]\n",
    "            label = self.labels[:, index, :]\n",
    "        \n",
    "        # Reshaping to 3D image\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        label = np.expand_dims(label, axis=0)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_inlines if self.orientation == 'in' else self.n_crosslines\n",
    "    \n",
    "    \n",
    "    def __load_data(self, data_path, labels_path):\n",
    "        if not os.path.isfile(data_path):\n",
    "            raise FileNotFoundError(f'File {data_path} does not exist.')\n",
    "        \n",
    "        if not os.path.isfile(labels_path):\n",
    "            raise FileNotFoundError(f'File {labels_path} does not exist.')\n",
    "        \n",
    "        _, data_extension = os.path.splitext(data_path)\n",
    "        \n",
    "        # Loading data\n",
    "        if data_extension in ['.segy', '.sgy']:\n",
    "            inlines = []\n",
    "        \n",
    "            with segyio.open(data_path, 'r') as segyfile:\n",
    "                segyfile.mmap()\n",
    "\n",
    "                for inline in segyfile.ilines:\n",
    "                    inlines.append(segyfile.iline[inline])\n",
    "\n",
    "            data = np.array(inlines)\n",
    "        else:\n",
    "            data = np.load(data_path)\n",
    "\n",
    "        # Loading labels\n",
    "        labels = np.load(labels_path)\n",
    "        \n",
    "        return data, labels\n",
    "\n",
    "\n",
    "    def __compute_class_weights(self):\n",
    "        total_n_values = self.n_inlines * self.n_crosslines * self.n_time_slices\n",
    "        # Weights are inversely proportional to the frequency of the classes in the training set\n",
    "        _, counts = np.unique(self.labels, return_counts=True)\n",
    "        \n",
    "        return total_n_values / (counts*self.n_classes)\n",
    "    \n",
    "\n",
    "    def __remove_faulty_slices(self, faulty_slices_list):\n",
    "        try:\n",
    "            with open(faulty_slices_list, 'r') as json_buffer:\n",
    "                # File containing the list of slices to delete\n",
    "                faulty_slices = json.loads(json_buffer.read())\n",
    "\n",
    "                self.data = np.delete(self.data, obj=faulty_slices['inlines'], axis=0)\n",
    "                self.data = np.delete(self.data, obj=faulty_slices['crosslines'], axis=1)\n",
    "                self.data = np.delete(self.data, obj=faulty_slices['time_slices'], axis=2)\n",
    "\n",
    "                self.labels = np.delete(self.labels, obj=faulty_slices['inlines'], axis=0)\n",
    "                self.labels = np.delete(self.labels, obj=faulty_slices['crosslines'], axis=1)\n",
    "                self.labels = np.delete(self.labels, obj=faulty_slices['time_slices'], axis=2)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print('Could not open the .json file containing the faulty slices.')\n",
    "            print('Training with the whole volume instead.\\n')\n",
    "\n",
    "            pass\n",
    "    \n",
    "\n",
    "    def __process_class_labels(self):\n",
    "        # Labels must be in the range [0, number_of_classes) for the loss function to work properly\n",
    "        label_values = np.unique(self.labels)\n",
    "        new_labels_dict = {label_values[i]: i for i in range(len(label_values))}\n",
    "\n",
    "        for key, value in zip(new_labels_dict.keys(), new_labels_dict.values()):\n",
    "            self.labels[self.labels == key] = value\n",
    "        \n",
    "        return len(label_values)\n",
    "    \n",
    "\n",
    "    def get_class_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "\n",
    "    def get_n_classes(self):\n",
    "        return self.n_classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
